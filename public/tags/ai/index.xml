<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Keepwalking</title>
    <link>https://huangbaihua001.github.io/tags/ai/</link>
    <description>Recent content in AI on Keepwalking</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2022, Keepwalking; all rights reserved. Powered by Hugo</copyright>
    <lastBuildDate>Tue, 18 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://huangbaihua001.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>初探 Transformer 背后的数学</title>
      <link>https://huangbaihua001.github.io/post/ai/llm/transfermor_math/</link>
      <pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>https://huangbaihua001.github.io/post/ai/llm/transfermor_math/</guid>
      <description>
        
          
            初探 Transformer 背后的数学 自2017年问世以来，Transformer 不仅是一种创新的模型架构，更是数学与工程精妙结合的典范。它通过自注意力机制（Se
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
